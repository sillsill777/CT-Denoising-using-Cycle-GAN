{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e7b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba45870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_layer(norm_type='instance'):\n",
    "    if norm_type=='instance':\n",
    "        norm_layer=functools.partial(nn.InstanceNorm2d,affine=False,track_running_stats=False)\n",
    "    elif norm_type=='batch':\n",
    "        norm_layer=functools.partial(nn.BatchNorm2d,affine=True,track_running_stats=True)\n",
    "    return norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(m,init_type='normal'):\n",
    "    name=m.__class__.__name__\n",
    "    if name.find('Conv') != -1:\n",
    "        if init_type=='normal':\n",
    "            init.normal_(m.weight.data,0.0,0.02)\n",
    "        elif init_type=='xavier':\n",
    "            init.xavier_normal_(m.weight.data,gain=0.02)\n",
    "        elif init_type=='kaiming':\n",
    "            init.kaiming_normal_(m.weight.data,a=0,mode='fan_in')\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias.data,0.0)\n",
    "    elif name.find('Batch') != -1:\n",
    "        init.normal_(m.weight.data,1.0,0.02)\n",
    "        init.constant_(m.bias.data,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8be6620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetSkipConnectionLayer(nn.Module):\n",
    "    def __init__(self,out_c,inner_c,in_c=None,outter=False,inner=False,use_drop=False,\n",
    "                 norm_layer=nn.BatchNorm2d,submodule=False):\n",
    "        super().__init__()\n",
    "        self.outter=outter\n",
    "        \n",
    "        if in_c is None:\n",
    "            in_c=out_c\n",
    "            \n",
    "        if type(norm_layer)==functools.partial:\n",
    "            use_bias=(norm_layer.func==nn.InstanceNorm2d)\n",
    "        else:\n",
    "            use_bias=(norm_layer==nn.InstanceNorm2d)\n",
    "        \n",
    "        downrelu=nn.LeakyReLU(0.2,True)\n",
    "        downConv=nn.Conv2d(in_c,inner_c,kernel_size=4,stride=2,padding=1,bias=use_bias)\n",
    "        uprelu=nn.ReLU(True)\n",
    "        upNorm=norm_layer(out_c)\n",
    "        downNorm=norm_layer(inner_c)\n",
    "        \n",
    "        if inner:\n",
    "            upConv=nn.ConvTranspose2d(inner_c,out_c,kernel_size=4,stride=2,padding=1,bias=use_bias)\n",
    "            down=[downrelu,downConv]\n",
    "            up=[uprelu,upConv,upNorm]\n",
    "            layers=down+up\n",
    "            \n",
    "        elif outter:\n",
    "            upConv=nn.ConvTranspose2d(inner_c*2,out_c,kernel_size=4,stride=2,padding=1)\n",
    "            down=[downConv]\n",
    "            up=[uprelu,upConv,nn.Tanh()]\n",
    "            layers=down+[submodule]+up\n",
    "        else:\n",
    "            upConv=nn.ConvTranspose2d(inner_c*2,out_c,kernel_size=4,stride=2,padding=1,bias=use_bias)\n",
    "            down=[downrelu,downConv,downNorm]\n",
    "            up=[uprelu,upConv,upNorm]\n",
    "            if use_drop:\n",
    "                layers=down+[submodule]+up+[nn.Dropout(0.5)]\n",
    "            else:\n",
    "                layers=down+[submodule]+up\n",
    "            \n",
    "        self.model=nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.outter:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([x,self.model(x)],dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "818a2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self,in_c,out_c,num_downs=8,ngf=64,norm_layer=nn.BatchNorm2d,use_drop=False):\n",
    "        super().__init__()\n",
    "        block=UNetSkipConnectionLayer(8*ngf,8*ngf,inner=True,norm_layer=norm_layer)\n",
    "        for i in range(num_downs-5):\n",
    "            block=UNetSkipConnectionLayer(8*ngf,8*ngf,norm_layer=norm_layer,use_drop=use_drop,submodule=block)\n",
    "        block=UNetSkipConnectionLayer(4*ngf,8*ngf,norm_layer=norm_layer,submodule=block)\n",
    "        block=UNetSkipConnectionLayer(2*ngf,4*ngf,norm_layer=norm_layer,submodule=block)\n",
    "        block=UNetSkipConnectionLayer(ngf,2*ngf,norm_layer=norm_layer,submodule=block)\n",
    "        self.model=UNetSkipConnectionLayer(out_c,ngf,in_c=in_c,norm_layer=norm_layer,submodule=block,outter=True)\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8cb14e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,in_c,ndf=64,n_layers=3,norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        if type(norm_layer)==functools.partial:\n",
    "            use_bias=norm_layer.func==nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias=norm_layer==nn.InstanceNorm2d\n",
    "        layers=[\n",
    "            nn.Conv2d(in_c,ndf,kernel_size=4,stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2,True)\n",
    "        ]\n",
    "        n_prev=1\n",
    "        n_cur=1\n",
    "        for i in range(1,n_layers):\n",
    "            n_cur=min(8,2**i)\n",
    "            layers+=[\n",
    "                nn.Conv2d(n_prev*ndf,n_cur*ndf,kernel_size=4,stride=2,padding=1,bias=use_bias),\n",
    "                norm_layer(n_cur*ndf),\n",
    "                nn.LeakyReLU(0.2,True)\n",
    "            ]\n",
    "            n_prev=n_cur\n",
    "        n_cur=min(2**n_layers,8)\n",
    "        layers+=[\n",
    "            nn.Conv2d(n_prev*ndf,n_cur*ndf,kernel_size=4,stride=1,padding=1,bias=use_bias),\n",
    "            norm_layer(n_cur*ndf),\n",
    "            nn.LeakyReLU(0.2,True)\n",
    "        ]\n",
    "        layers+=[\n",
    "            nn.Conv2d(n_cur*ndf,1,kernel_size=4,stride=1,padding=1)\n",
    "        ]\n",
    "        self.model=nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a538c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
